\relax 
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Analysis}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Behavioral Cloning}{3}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Evaluation Mean and Standard Deviation of my policy's return over multiple rollouts, on the Ant and Walker2d tasks.} I set \texttt  {seed} to 4, \texttt  {ep\_len} to 1000, and \texttt  {eval\_batch\_size} to 10000. All other hyperparameters (e.g. model architecture, learning rate, etc.) are identical. We observe very different results for the Ant and HalfCheetah tasks, even though the training configurations used were the exact same. In particular, our policy was able to achieve 81.2\% of the performance of the expert on the Ant task, and only 12.1\% on the Walker2d task.}}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {BC Agent Evaluation Average Return on the HalfCheetah task vs. Number of MLP Hidden Layers.} I varied the hidden layer from 2 to 12, and kept all other training configurations the same as in part (1). We can see that the BC's performance fluctuates a little bit depending on the hidden layer count, which is a bit unexpected. I anticipated seeing a unimodal distributionâ€”hidden layers should improve the agent's ability to learn complex behavior, up to a certain point where overfitting occurs. The bimodal result we see here demonstrates similar ideas but the "tip-off" point is much less defined.}}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}\textsc  {DAgger}}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{4}{}\protected@file@percent }
\gdef \@abspage@last{4}
